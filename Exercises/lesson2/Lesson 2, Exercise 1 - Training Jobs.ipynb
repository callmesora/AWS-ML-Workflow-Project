{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "345c18af",
   "metadata": {},
   "source": [
    "# UDACITY SageMaker Essentials: Training Job Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea8b4d8",
   "metadata": {},
   "source": [
    "Good job on your work so far! You've gotten an overview of building an ML Workflow in AWS. Now, it's time to practice your skills. In this exercise, you will be training a BlazingText model to help predict the helpfulness of Amazon reviews. The model & parameters have already been chosen for you; it's your task to properly upload the data necessary for the job and launch the training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9e6bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16340311",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d83e2cb",
   "metadata": {},
   "source": [
    "The data we'll be examining today is a collection of reviews for an assortment of toys and games found on Amazon. This data includes, but is not limited to, the text of the review itself as well as the number of user \"votes\" on whether or not the review was helpful. Today, we will be making a model that predicts the usefulness of a review, given only the text of the review. This is an example of a problem in the domain of supervised sentiment analysis; we are trying to extract something subjective from text given prior labeled text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea38f892",
   "metadata": {},
   "source": [
    "Before we get started, we want to know what form of data is accepted in the algorithm we're using. We'll be using BlazingText, an implemention of Word2Vec optimized for SageMaker. In order for this optimization to be effective, data needs to be preprocessed to match the correct format. The documentation for this algorithm can be found here: https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59be0c47",
   "metadata": {},
   "source": [
    "We will be training under \"File Mode\", which requires us to do two things in preprocessing this data. First, we need to generate labels from the votes. For this exercise, if the majority of votes for a review is helpful, we will designate it \\_\\_label\\_\\_1, and if the majority of votes for a review is unhelpful, we will designate it \\_\\_label\\_\\_2. In the edge case where the values are equal, we will drop the review from consideration. Second, we need to separate the sentences, while keeping the original label for the review. These reviews will often consist of several sentences, and this algorithm is optimized to perform best on many small sentences rather than fewer larger paragraphs. We will separate these sentences by the character \".\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b79bab4",
   "metadata": {},
   "source": [
    "(This process is obviously very naive, but we will get remarkable results even without a lot of finetuning!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09129d9c",
   "metadata": {},
   "source": [
    "This preprocessing is done for you in the cells below. Make sure you go through the code and understand what's being done in each step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52fb5427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__label__1 Love the magnet easel', '__label__1  great for moving to different areas', '__label__1  Wish it had some sort of non skid pad on bottom though', '__label__1 Both sides are magnetic', \"__label__1  A real plus when you're entertaining more than one child\", '__label__1  The four-year old can find the letters for the words, while the two-year old can find the pictures the words spell', '__label__1  (I bought letters and magnetic pictures to go with this board)', '__label__1  Both grandkids liked it a lot, which means I like it a lot as well', '__label__1  Have not even introduced markers, as this will be used strictly as a magnetic board']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# Function below unzips the archive to the local directory. \n",
    "\n",
    "def unzip_data(input_data_path):\n",
    "    with zipfile.ZipFile(input_data_path, 'r') as input_data_zip:\n",
    "        input_data_zip.extractall('.')\n",
    "\n",
    "# Input data is a file with a single JSON object per line with the following format: \n",
    "# {\n",
    "#  \"reviewerID\": <string>,\n",
    "#  \"asin\": <string>,\n",
    "#  \"reviewerName\" <string>,\n",
    "#  \"helpful\": [\n",
    "#    <int>, (indicating number of \"helpful votes\")\n",
    "#    <int>  (indicating total number of votes)\n",
    "#  ],\n",
    "#  \"reviewText\": \"<string>\",\n",
    "#  \"overall\": <int>,\n",
    "#  \"summary\": \"<string>\",\n",
    "#  \"unixReviewTime\": <int>,\n",
    "#  \"reviewTime\": \"<string>\"\n",
    "# }\n",
    "# \n",
    "# We are specifically interested in the fields \"helpful\" and \"reviewText\"\n",
    "#\n",
    "\n",
    "def label_data(input_data):\n",
    "    labeled_data = []\n",
    "    HELPFUL_LABEL = \"__label__1\"\n",
    "    UNHELPFUL_LABEL = \"__label__2\"\n",
    "     \n",
    "    for l in open(input_data, 'r'):\n",
    "        l_object = json.loads(l)\n",
    "        helpful_votes = float(l_object['helpful'][0])\n",
    "        total_votes = l_object['helpful'][1]\n",
    "        reviewText = l_object['reviewText']\n",
    "        if total_votes != 0:\n",
    "            if helpful_votes / total_votes > .5:\n",
    "                labeled_data.append(\" \".join([HELPFUL_LABEL, reviewText]))\n",
    "            elif helpful_votes / total_votes < .5:\n",
    "                labeled_data.append(\" \".join([UNHELPFUL_LABEL, reviewText]))\n",
    "          \n",
    "    return labeled_data\n",
    "\n",
    "\n",
    "# Labeled data is a list of sentences, starting with the label defined in label_data. \n",
    "\n",
    "def split_sentences(labeled_data):\n",
    "    split_sentences = []\n",
    "    for d in labeled_data:\n",
    "        label = d.split()[0]        \n",
    "        sentences = \" \".join(d.split()[1:]).split(\".\") # Initially split to separate label, then separate sentences\n",
    "        for s in sentences:\n",
    "            if s: # Make sure sentences isn't empty. Common w/ \"...\"\n",
    "                split_sentences.append(\" \".join([label, s]))\n",
    "    return split_sentences\n",
    "\n",
    "\n",
    "input_data  = unzip_data('Toys_and_Games_5.json.zip')\n",
    "labeled_data = label_data('Toys_and_Games_5.json')\n",
    "split_sentence_data = split_sentences(labeled_data)\n",
    "\n",
    "print(split_sentence_data[0:9])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478252a7",
   "metadata": {},
   "source": [
    "## Exercise: Upload Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0724810e",
   "metadata": {},
   "source": [
    "Your first responsibility is to separate that `split_sentence_data` into a `training_file` and a `validation_file`. Have the training file make up 90% of the data, and have the validation file make up 10% of the data. Careful that the data doesn't overlap! (This will result in overfitting, which might result in nice validation metrics, but crummy generalization.)\n",
    "\n",
    "Using the methodology of your choice, upload these files to S3. (In practice, it's important to know how to do this through the console, programatically, and through the CLI. If you're feeling frisky, try all 3!) If you're doing this programatically, the Boto3 documentation would be a good reference. https://boto3.amazonaws.com/v1/documentation/api/latest/index.html\n",
    "\n",
    "The BUCKET will be the name of the bucket you wish to upload it to. The s3_prefix will be the name of the desired 'file-path' that you upload your file to within the bucket. For example, if you wanted to upload a file to:\n",
    "\n",
    "\"s3://example-bucket/1/2/3/example.txt\n",
    "\n",
    "The \"BUCKET\" will be 'example-bucket', and the s3_prefix would be '1/2/3'\n",
    "\n",
    "The code below shows you how to upload it programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6820e8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "# Note: This section implies that the bucket below has already been made and that you have access\n",
    "# to that bucket. You would need to change the bucket below to a bucket that you have write\n",
    "# premissions to. This will take time depending on your internet connection, the training file is ~ 40 mb\n",
    "\n",
    "BUCKET = \"CHANGE THIS\"\n",
    "s3_prefix = \"CHANGE THIS\"\n",
    "\n",
    "\n",
    "def cycle_data(fp, data):\n",
    "    for d in data:\n",
    "        fp.write(d + \"\\n\")\n",
    "\n",
    "def write_trainfile(split_sentence_data):\n",
    "    train_path = \"hello_blaze_train\"\n",
    "    with open(train_path, 'w') as f:\n",
    "        cycle_data(f, split_sentence_data)\n",
    "    return train_path\n",
    "\n",
    "def write_validationfile(split_sentence_data):\n",
    "    validation_path = \"hello_blaze_validation\"\n",
    "    with open(validation_path, 'w') as f:\n",
    "        cycle_data(f, split_sentence_data)\n",
    "    return validation_path \n",
    "\n",
    "def upload_file_to_s3(file_name, s3_prefix):\n",
    "    object_name = os.path.join(s3_prefix, file_name)\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, BUCKET, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    \n",
    "# Split the data\n",
    "split_data_trainlen = int(len(split_sentence_data) * .9)\n",
    "split_data_validationlen = int(len(split_sentence_data) * .1)\n",
    "\n",
    "# Todo: write the training file\n",
    "train_path = write_trainfile()\n",
    "print(\"Training file written!\")\n",
    "\n",
    "# Todo: write the validation file\n",
    "validation_path = write_validationfile()\n",
    "print(\"Validation file written!\")\n",
    "\n",
    "upload_file_to_s3(train_path, s3_prefix)\n",
    "print(\"Train file uploaded!\")\n",
    "upload_file_to_s3(validation_path, s3_prefix)\n",
    "print(\"Validation file uploaded!\")\n",
    "\n",
    "print(\" \".join([train_path, validation_path]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f103ba51",
   "metadata": {},
   "source": [
    "## Exercise: Train SageMaker Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd8b876",
   "metadata": {},
   "source": [
    "Believe it or not, you're already almost done! Part of the appeal of SageMaker is that AWS has already done the heavy implementation lifting for you. Launch a \"BlazingText\" training job from the SageMaker console. You can do so by searching \"SageMaker\", and navigating to Training Jobs on the left hand side. After selecting \"Create Training Job\", perform the following steps:\n",
    "* Select \"BlazingText\" from the algorithms available. \n",
    "* Specify the \"file\" input mode of training. \n",
    "* Under \"resource configuration\", select the \"ml.m5.large\" instance type. Specify 5 additional GBs of memory. \n",
    "* Set a stopping condition for 15 minutes. \n",
    "* Under hyperparameters, set \"mode\" to \"supervised\"\n",
    "* Under input_data configuration, input the S3 path to your training and validation datasets under the \"train\" and \"validation\" channels. You will need to create a channel named \"validation\".  \n",
    "* Specify an output path in the same bucket that you uploaded training and validation data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a48c1d",
   "metadata": {},
   "source": [
    "If successful, you should see a training job launch in the UI. Go grab a coffee, this will take a little bit of time. If there was a failure, you should see it there. Googling the error should direct "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2604071",
   "metadata": {},
   "source": [
    "## Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429bb72f",
   "metadata": {},
   "source": [
    "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering  \n",
    "R. He, J. McAuley  \n",
    "WWW, 2016\n",
    "\n",
    "\n",
    "Image-based recommendations on styles and substitutes  \n",
    "J. McAuley, C. Targett, J. Shi, A. van den Hengel  \n",
    "SIGIR, 2015\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
