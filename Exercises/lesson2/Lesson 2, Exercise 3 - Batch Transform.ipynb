{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e342763",
   "metadata": {},
   "source": [
    "# UDACITY SageMaker Essentials: Batch Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b3eee",
   "metadata": {},
   "source": [
    "In the last exercise, we asked you to reflect on the disadvantages of having to perform preprocessing on a local machine. In addition to those disadvantages, such as user error and hardware limitations, you may have also encountered another frustration in submitting a large amount of data to an endpoint. There may be network limitations on your end, there may be security/privacy concerns, and there might be an obvious performance advantage in parallelism that may be difficult to implement. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc794fe",
   "metadata": {},
   "source": [
    "Batch transform essentially addresses all of these issues. The primary use case for this is to make an inference on a dataset rather than making many individual calls to an endpoint. AWS SageMaker, similar to other tools that we encountered, does the heavy implementation lifting of reading data and splitting the burden among instances. All that's required of us is to give batch transform the correct directions to the data we want to submit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae54cdc1",
   "metadata": {},
   "source": [
    "Alas, this dataset is unfortunately not quite in the correct format to be properly digested by batch transform. Although this tool is capable of digesting lists of json objects, it is not capable of the processing operations that we would ideally perform on it. So, yet again, we must preprocess data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887aa98f",
   "metadata": {},
   "source": [
    "## Exercise: Preprocess (again, again) and upload to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692a223d",
   "metadata": {},
   "source": [
    "The cell below provides you two functions. The `split_sentences` preprocesses the reviews and you should be very familiar with function. Remember that the BlazingText expects a input with JSON format, the `cycle_data` formats the review to the following: {'source': 'THIS IS A SAMPLE SENTENCE'} and writes it into a file.\n",
    "\n",
    "Using the cell to complete the following tasks:\n",
    "* preprecessing reviews_Musical_Instruments_5.json \n",
    "* upload the file consisting of the data to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03829953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Todo: Input the s3 bucket\n",
    "s3_bucket = \"CHANGE THIS\"\n",
    "\n",
    "# Todo: Input the s3 prefix\n",
    "s3_prefix = \"CHANGE THIS\"\n",
    "\n",
    "# Todo: Input the the file to write the data to\n",
    "file_name = \"CHANGE THIS\"\n",
    "\n",
    "# Function below unzips the archive to the local directory. \n",
    "\n",
    "def unzip_data(input_data_path):\n",
    "    with zipfile.ZipFile(input_data_path, 'r') as input_data_zip:\n",
    "        input_data_zip.extractall('.')\n",
    "\n",
    "\n",
    "def split_sentences(input_data):\n",
    "    split_sentences = []\n",
    "    for l in open(input_data, 'r'):\n",
    "        l_object = json.loads(l)\n",
    "        helpful_votes = float(l_object['helpful'][0])\n",
    "        total_votes = l_object['helpful'][1]\n",
    "        if total_votes != 0 and helpful_votes/total_votes != .5:  # Filter out same data as prior jobs. \n",
    "            reviewText = l_object['reviewText']\n",
    "            sentences = reviewText.split(\".\") \n",
    "            for s in sentences:\n",
    "                if s: # Make sure sentences isn't empty. Common w/ \"...\"\n",
    "                    split_sentences.append(s)\n",
    "    return split_sentences\n",
    "\n",
    "# Format the data as {'source': 'THIS IS A SAMPLE SENTENCE'}\n",
    "# And write the data into a file\n",
    "def cycle_data(fp, data):\n",
    "    for d in data:\n",
    "        fp.write(json.dumps({'source':d}) + '\\n')\n",
    "\n",
    "# Todo: write a function to upload the data to s3\n",
    "def upload_file_to_s3(file_name, s3_prefix):\n",
    "    return\n",
    "\n",
    "\n",
    "# Unzips file.\n",
    "unzip_data('reviews_Musical_Instruments_5.json.zip')\n",
    "\n",
    "# Todo: preprocess reviews_Musical_Instruments_5.json \n",
    "sentences = split_sentences('')\n",
    "\n",
    "# Write data to a file and upload it to s3.\n",
    "with open(file_name, 'w') as f:\n",
    "    cycle_data(f, sentences)\n",
    "\n",
    "upload_file_to_s3(file_name, s3_prefix)\n",
    "\n",
    "# Get the s3 path for the data\n",
    "batch_transform_input_path = \"s3://\" + \"/\".join([s3_bucket, s3_prefix, file_name])\n",
    "\n",
    "print(batch_transform_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724938c",
   "metadata": {},
   "source": [
    "## Exercise: Use Batch Transform to perform an inference on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b34d68",
   "metadata": {},
   "source": [
    "We utilize batch transform through a transformer object. Similar to how we initialized a predictor object in the last exercise, complete the code below to initialize a transformer object and launch a transform job.   \n",
    "\n",
    "You will need the following:\n",
    "\n",
    "* Similar to last exercise, you will need to get a BlazingText image uri from AWS. The methodology you use to do so should be identical to the last exercise.  \n",
    "* You will need to instantiate a \"model\" object.\n",
    "* You will need to call the \"transformer\" method on the model object to create a transformer. We suggest using 1 instance of ml.m4.xlarge. If this isn't available in your region, feel free to use another instance, such as ml.m5.large\n",
    "* You will need to use this transformer on the data we uploaded to s3. You will be able to do so by inserting an \"S3Prefix\" data_type and a \"application/jsonlines\" content_type, split by \"Line\".\n",
    "\n",
    "Consult the following documentation: https://sagemaker.readthedocs.io/en/stable/api/inference/transformer.html\n",
    "\n",
    "End-to-end, this process should take about 5 minutes on the whole dataset. While developing, consider uploading a subset of the data to s3, and evaluate on that instead. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c8c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import image_uris\n",
    "\n",
    "# Get the execution role\n",
    "\n",
    "role = \n",
    "\n",
    "# Get the image uri using the \"blazingtext\" algorithm in your region. \n",
    "\n",
    "image_uri = \n",
    "\n",
    "# Get the model artifact from S3\n",
    "\n",
    "model_data = \n",
    "\n",
    "# Get the s3 path for the batch transform data\n",
    "\n",
    "batch_transform_output_path = \n",
    "\n",
    "# Define a model object\n",
    "\n",
    "model = \n",
    "\n",
    "# Define a transformer object, using a single instance ml.m4.xlarge. Specify an output path to your s3 bucket. \n",
    "\n",
    "transformer = \n",
    "\n",
    "# Call the transform method. Set content_type='application/jsonlines', split_type='Line'\n",
    "\n",
    "transformer.transform(\n",
    "    data=, \n",
    "    data_type=,\n",
    "    content_type=, \n",
    "    split_type=\n",
    ")\n",
    "\n",
    "transformer.wait()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ab01a",
   "metadata": {},
   "source": [
    "## Exercise: Sanity Check - Are Results the Same? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a68319e",
   "metadata": {},
   "source": [
    "The results of the inference should be printed to the s3 path specified in batch_transform_output_path. We have evaluated the same data on the same model, so if all is done correctly on both exercises, the inferences should be the same. Compare the first five or so inferences on the last exercise and on this exercise to confirm this. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
